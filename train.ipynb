{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9702dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tetris import Tetris\n",
    "from tetris_master import TetrisMaster2\n",
    "from Agent import DQNAgent\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5044930",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = 10, 20\n",
    "game_len = 5\n",
    "epoch = 10000\n",
    "betta = 0.3\n",
    "gamma = 0.89\n",
    "rf = 4\n",
    "lr = 1e-5\n",
    "epsilon, epsilon_min, epsilon_decay = 0.4, 0., 0.995\n",
    "batch_size = 10\n",
    "\n",
    "train_ep = 100\n",
    "test_ep = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708a92db",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 9 required positional arguments: 'model', 'state_dim', 'action_dim', 'learning_rate', 'betta', 'gamma', 'epsilon', 'epsilon_min', and 'epsilon_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m cp_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m TetrisMaster2(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m---> 11\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 9 required positional arguments: 'model', 'state_dim', 'action_dim', 'learning_rate', 'betta', 'gamma', 'epsilon', 'epsilon_min', and 'epsilon_decay'"
     ]
    }
   ],
   "source": [
    "env = Tetris(w, h)\n",
    "\n",
    "# Initialize the agent\n",
    "state_dim = (env.board.shape, 1)\n",
    "action_dim = 4\n",
    "\n",
    "cp_path = 'checkpoints'\n",
    "\n",
    "model = TetrisMaster2(8, 32, 100).double()\n",
    "\n",
    "agent = DQNAgent(\n",
    "    model, state_dim, action_dim, lr, betta, gamma, epsilon, epsilon_min, epsilon_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a7a02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:37:33<00:00,  5.85s/it, str=Episode: 1000, loss 0.2568, Total Reward: -0.0086]\n",
      "100%|██████████| 10/10 [02:48<00:00, 16.88s/it, str=Test Episode, loss 0.2568, Total Reward: -0.0165]\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Training the DQN agent\n",
    "    train_iter = tqdm(range(train_ep))\n",
    "    model.train()\n",
    "    for episode in train_iter:\n",
    "        with torch.no_grad():\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            iters = 0\n",
    "            while iters < game_len and not done:\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward = total_reward + reward\n",
    "                iters += 1\n",
    "        loss = agent.replay(batch_size)\n",
    "        train_iter.set_postfix(str=f\"loss {loss.item():.5}, \" + f\"{iters=}, \" +\n",
    "                        f\"rwd: {round(total_reward, rf)}, \" +\n",
    "                        f\"eps {round(agent.epsilon, rf)}\")\n",
    "        \n",
    "    # Evaluate the trained agent\n",
    "    test_iter = tqdm(range(test_ep))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in test_iter:\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            iters = 0\n",
    "            # while iters < game_len and not done:\n",
    "            while iters < 100 and not done:\n",
    "                action = agent.select_action(state, test=True)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                iters += 1\n",
    "            test_iter.set_postfix(str=f\"Test Episode, loss {round(loss.item(), rf)}, \" +\n",
    "                        f\"Total Reward: {round(total_reward, rf)}\")\n",
    "    test_iter.close()\n",
    "\n",
    "    # print('saving')\n",
    "    try:\n",
    "        torch.save({\n",
    "            'model_state_dict': agent.online_net.state_dict(),\n",
    "        }, os.path.join(cp_path, 'test_model'))\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save({\n",
    "            'model_state_dict': agent.online_net.state_dict(),\n",
    "        }, os.path.join(cp_path, 'test_model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8bd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-jupyter",
   "language": "python",
   "name": "ML-jyputer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
