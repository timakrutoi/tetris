{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9702dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tetris import Tetris, rewards\n",
    "from tetris_master import TetrisMaster2\n",
    "from Agent import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5044930",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = 10, 20\n",
    "game_len = 50\n",
    "epoch = 10000\n",
    "betta = 0.3\n",
    "gamma = 0.89\n",
    "rf = 4\n",
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708a92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Tetris(w, h)\n",
    "\n",
    "# Initialize the agent\n",
    "state_dim = (env.board.shape, 1)\n",
    "action_dim = w, 4\n",
    "batch_size = 1\n",
    "\n",
    "model = TetrisMaster2(bw=w, bh=h).double()\n",
    "\n",
    "agent = DQNAgent(model, state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad5a7a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                | 4/1000 [00:00<00:30, 32.34it/s, str=Episode: 7, loss 18.6632, Total Reward: -0.0202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                               | 8/1000 [00:00<00:28, 34.61it/s, str=Episode: 11, loss 14.1125, Total Reward: -0.0202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                              | 11/1000 [00:14<21:52,  1.33s/it, str=Episode: 11, loss 14.1125, Total Reward: -0.0202]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      9\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m---> 10\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/projects/tetris/tetris.py:62\u001b[0m, in \u001b[0;36mTetris.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m---> 62\u001b[0m     rwds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     63\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m rwds \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlose_game_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext), rwds, done\n",
      "File \u001b[0;32m~/projects/tetris/tetris.py:151\u001b[0m, in \u001b[0;36mTetris.turn\u001b[0;34m(self, col, rot)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# rotate piece\u001b[39;00m\n\u001b[1;32m    149\u001b[0m cur_tetrimino \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrot90(\u001b[38;5;28mlist\u001b[39m(tetriminos\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext], rot)\n\u001b[0;32m--> 151\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplace_piece\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_tetrimino\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m rp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    153\u001b[0m rr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/projects/tetris/tetris.py:95\u001b[0m, in \u001b[0;36mTetris.place_piece\u001b[0;34m(self, col, cur_tetrimino)\u001b[0m\n\u001b[1;32m     92\u001b[0m             next_board \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#             print(get_height(self.board) + h, self.height)\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mget_height\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m h \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight:\n\u001b[1;32m     96\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlose_game_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlose_game_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/tetris/tetris.py:264\u001b[0m, in \u001b[0;36mget_height\u001b[0;34m(buffer)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_height\u001b[39m(buffer):\n\u001b[0;32m--> 264\u001b[0m     hs \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39margmax(buffer[:,i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(buffer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m    265\u001b[0m     hs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hs)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(hs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/projects/tetris/tetris.py:264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_height\u001b[39m(buffer):\n\u001b[0;32m--> 264\u001b[0m     hs \u001b[38;5;241m=\u001b[39m [(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(buffer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m    265\u001b[0m     hs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hs)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(hs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the DQN agent\n",
    "episodes = 1000\n",
    "it = tqdm(range(episodes))\n",
    "for episode in it:\n",
    "    state = env.clear_board()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    print('Done')\n",
    "    loss = agent.replay(batch_size)\n",
    "    it.set_postfix(str=f\"Episode: {episode + 1}, loss {round(loss, rf)}, Total Reward: {round(total_reward, rf)}\")\n",
    "it.close()\n",
    "\n",
    "# Evaluate the trained agent\n",
    "test_episodes = 10\n",
    "it = tqdm(range(test_episodes))\n",
    "for _ in it:\n",
    "    state = env.clear_board()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    it.set_postfix(str=f\"Test Episode, loss {round(loss, rf)}, Total Reward: {round(total_reward, rf)}\")\n",
    "\n",
    "try:\n",
    "    torch.save({\n",
    "        'model_state_dict': agent.model.state_dict(),\n",
    "    }, 'checkpoints/test_model'.format(epoch, best_loss))\n",
    "except KeyboardInterrupt:\n",
    "    torch.save({\n",
    "        'model_state_dict': agent.model.state_dict(),\n",
    "    }, 'checkpoints/test_model'.format(epoch, best_loss))\n",
    "\n",
    "it.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8bd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-jupyter",
   "language": "python",
   "name": "ml-jyputer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
